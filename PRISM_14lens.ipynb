{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîÆ PRISM Engine - Full 14 Lenses\n",
    "\n",
    "One notebook. All lenses. No imports needed.\n",
    "\n",
    "**Run cells 1-4 in order, then explore.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ‚öôÔ∏è **CELL 1: SETUP** (run first) { display-mode: \"form\" }\n",
    "#@markdown Mounts Drive, installs packages, loads all 14 lenses.\n",
    "\n",
    "# Mount Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Config - edit if your path differs\n",
    "PRISM_PATH = '/content/drive/MyDrive/prism-engine/prism-engine'\n",
    "\n",
    "# Imports\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.insert(0, PRISM_PATH)\n",
    "\n",
    "# ============================================================================\n",
    "# ALL 14 LENSES - SELF-CONTAINED\n",
    "# ============================================================================\n",
    "\n",
    "class BaseLens:\n",
    "    name = \"base\"\n",
    "    description = \"Base lens class\"\n",
    "    def analyze(self, panel): raise NotImplementedError\n",
    "\n",
    "\n",
    "# --- LENS 1: MAGNITUDE ---\n",
    "class MagnitudeLens(BaseLens):\n",
    "    \"\"\"Importance by total magnitude of movement (L2 norm).\"\"\"\n",
    "    name = \"magnitude\"\n",
    "    def analyze(self, panel):\n",
    "        normalized = (panel - panel.mean()) / panel.std()\n",
    "        magnitude = np.sqrt((normalized ** 2).sum())\n",
    "        return {'importance': magnitude}\n",
    "\n",
    "\n",
    "# --- LENS 2: PCA ---\n",
    "class PCALens(BaseLens):\n",
    "    \"\"\"Importance by contribution to principal components.\"\"\"\n",
    "    name = \"pca\"\n",
    "    def analyze(self, panel, n_components=5):\n",
    "        X = ((panel - panel.mean()) / panel.std()).fillna(0).values\n",
    "        U, S, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "        explained = (S ** 2) / (len(X) - 1)\n",
    "        loadings = Vt[:n_components].T * S[:n_components]\n",
    "        importance = pd.Series(np.abs(loadings).sum(axis=1), index=panel.columns)\n",
    "        return {'importance': importance, 'explained_variance': explained[:n_components] / explained.sum()}\n",
    "\n",
    "\n",
    "# --- LENS 3: GRANGER CAUSALITY ---\n",
    "class GrangerLens(BaseLens):\n",
    "    \"\"\"Importance by Granger-causal influence on other series.\"\"\"\n",
    "    name = \"granger\"\n",
    "    def analyze(self, panel, max_lag=5):\n",
    "        cols = panel.columns.tolist()\n",
    "        influence = pd.Series(0.0, index=cols)\n",
    "        \n",
    "        for target in cols:\n",
    "            y = panel[target].values\n",
    "            for source in cols:\n",
    "                if source == target: continue\n",
    "                x = panel[source].values\n",
    "                \n",
    "                # Simple Granger: does lagged X improve prediction of Y?\n",
    "                n = len(y) - max_lag\n",
    "                if n < 20: continue\n",
    "                \n",
    "                # Restricted model: Y on lagged Y only\n",
    "                Y = y[max_lag:]\n",
    "                X_restricted = np.column_stack([y[max_lag-i-1:-i-1] for i in range(max_lag)])\n",
    "                \n",
    "                # Unrestricted: add lagged X\n",
    "                X_unrestricted = np.column_stack([X_restricted] + [x[max_lag-i-1:-i-1] for i in range(max_lag)])\n",
    "                \n",
    "                try:\n",
    "                    # OLS\n",
    "                    rss_r = np.sum((Y - X_restricted @ np.linalg.lstsq(X_restricted, Y, rcond=None)[0])**2)\n",
    "                    rss_u = np.sum((Y - X_unrestricted @ np.linalg.lstsq(X_unrestricted, Y, rcond=None)[0])**2)\n",
    "                    \n",
    "                    # F-stat\n",
    "                    f_stat = ((rss_r - rss_u) / max_lag) / (rss_u / (n - 2*max_lag))\n",
    "                    if f_stat > 2:  # Rough significance\n",
    "                        influence[source] += f_stat\n",
    "                except: pass\n",
    "        \n",
    "        importance = (influence - influence.min()) / (influence.max() - influence.min() + 1e-10)\n",
    "        return {'importance': importance, 'granger_influence': influence}\n",
    "\n",
    "\n",
    "# --- LENS 4: DMD (Dynamic Mode Decomposition) ---\n",
    "class DMDLens(BaseLens):\n",
    "    \"\"\"Importance by contribution to dominant dynamic modes.\"\"\"\n",
    "    name = \"dmd\"\n",
    "    def analyze(self, panel, n_modes=5):\n",
    "        X = ((panel - panel.mean()) / panel.std()).fillna(0).values\n",
    "        X1, X2 = X[:-1].T, X[1:].T\n",
    "        \n",
    "        # DMD via SVD\n",
    "        U, S, Vh = np.linalg.svd(X1, full_matrices=False)\n",
    "        r = min(n_modes, len(S))\n",
    "        U, S, Vh = U[:, :r], S[:r], Vh[:r, :]\n",
    "        \n",
    "        # DMD matrix\n",
    "        Atilde = U.T @ X2 @ Vh.T @ np.diag(1/S)\n",
    "        eigs, W = np.linalg.eig(Atilde)\n",
    "        \n",
    "        # Modes\n",
    "        Phi = X2 @ Vh.T @ np.diag(1/S) @ W\n",
    "        \n",
    "        # Importance: sum of mode amplitudes\n",
    "        importance = pd.Series(np.abs(Phi).sum(axis=1), index=panel.columns)\n",
    "        return {'importance': importance, 'eigenvalues': eigs}\n",
    "\n",
    "\n",
    "# --- LENS 5: INFLUENCE ---\n",
    "class InfluenceLens(BaseLens):\n",
    "    \"\"\"Importance by volatility √ó deviation from mean.\"\"\"\n",
    "    name = \"influence\"\n",
    "    def analyze(self, panel, window=20):\n",
    "        vol = panel.rolling(window=window).std()\n",
    "        dev = np.abs(panel - panel.rolling(window=window).mean())\n",
    "        influence = (vol * dev).mean()\n",
    "        importance = (influence - influence.min()) / (influence.max() - influence.min() + 1e-10)\n",
    "        return {'importance': importance}\n",
    "\n",
    "\n",
    "# --- LENS 6: MUTUAL INFORMATION ---\n",
    "class MutualInfoLens(BaseLens):\n",
    "    \"\"\"Importance by mutual information with other series.\"\"\"\n",
    "    name = \"mutual_info\"\n",
    "    def analyze(self, panel, n_bins=20):\n",
    "        cols = panel.columns.tolist()\n",
    "        mi_scores = pd.Series(0.0, index=cols)\n",
    "        \n",
    "        for i, col1 in enumerate(cols):\n",
    "            x = panel[col1].values\n",
    "            for col2 in cols[i+1:]:\n",
    "                y = panel[col2].values\n",
    "                \n",
    "                # Discretize\n",
    "                x_bins = np.digitize(x, np.linspace(x.min(), x.max(), n_bins))\n",
    "                y_bins = np.digitize(y, np.linspace(y.min(), y.max(), n_bins))\n",
    "                \n",
    "                # Joint and marginal histograms\n",
    "                pxy = np.histogram2d(x_bins, y_bins, bins=n_bins)[0]\n",
    "                pxy = pxy / pxy.sum()\n",
    "                px = pxy.sum(axis=1)\n",
    "                py = pxy.sum(axis=0)\n",
    "                \n",
    "                # MI\n",
    "                mi = 0\n",
    "                for xi in range(n_bins):\n",
    "                    for yi in range(n_bins):\n",
    "                        if pxy[xi, yi] > 0 and px[xi] > 0 and py[yi] > 0:\n",
    "                            mi += pxy[xi, yi] * np.log(pxy[xi, yi] / (px[xi] * py[yi]))\n",
    "                \n",
    "                mi_scores[col1] += mi\n",
    "                mi_scores[col2] += mi\n",
    "        \n",
    "        importance = (mi_scores - mi_scores.min()) / (mi_scores.max() - mi_scores.min() + 1e-10)\n",
    "        return {'importance': importance, 'mutual_info': mi_scores}\n",
    "\n",
    "\n",
    "# --- LENS 7: CLUSTERING ---\n",
    "class ClusteringLens(BaseLens):\n",
    "    \"\"\"Importance by centrality within correlation clusters.\"\"\"\n",
    "    name = \"clustering\"\n",
    "    def analyze(self, panel):\n",
    "        corr = panel.corr().abs()\n",
    "        # Importance = average correlation with others (centrality)\n",
    "        importance = (corr.sum() - 1) / (len(corr) - 1)\n",
    "        return {'importance': importance, 'correlation_matrix': corr}\n",
    "\n",
    "\n",
    "# --- LENS 8: DECOMPOSITION ---\n",
    "class DecompositionLens(BaseLens):\n",
    "    \"\"\"Importance by trend vs noise ratio.\"\"\"\n",
    "    name = \"decomposition\"\n",
    "    def analyze(self, panel, period=252):\n",
    "        importance_vals = []\n",
    "        for col in panel.columns:\n",
    "            s = panel[col].dropna()\n",
    "            if len(s) < period * 2:\n",
    "                importance_vals.append(0)\n",
    "                continue\n",
    "            trend = s.rolling(window=period, center=True).mean()\n",
    "            trend_var = trend.var()\n",
    "            total_var = s.var()\n",
    "            importance_vals.append(trend_var / total_var if total_var > 0 else 0)\n",
    "        importance = pd.Series(importance_vals, index=panel.columns)\n",
    "        return {'importance': importance}\n",
    "\n",
    "\n",
    "# --- LENS 9: WAVELET ---\n",
    "class WaveletLens(BaseLens):\n",
    "    \"\"\"Importance by multi-scale variance (wavelet-like decomposition).\"\"\"\n",
    "    name = \"wavelet\"\n",
    "    def analyze(self, panel, scales=[5, 20, 60, 120, 252]):\n",
    "        importance_vals = []\n",
    "        for col in panel.columns:\n",
    "            s = panel[col].dropna()\n",
    "            scale_vars = []\n",
    "            for scale in scales:\n",
    "                if len(s) > scale:\n",
    "                    smoothed = s.rolling(window=scale).mean()\n",
    "                    detail = s - smoothed\n",
    "                    scale_vars.append(detail.var())\n",
    "            importance_vals.append(np.mean(scale_vars) if scale_vars else 0)\n",
    "        importance = pd.Series(importance_vals, index=panel.columns)\n",
    "        importance = (importance - importance.min()) / (importance.max() - importance.min() + 1e-10)\n",
    "        return {'importance': importance}\n",
    "\n",
    "\n",
    "# --- LENS 10: NETWORK ---\n",
    "class NetworkLens(BaseLens):\n",
    "    \"\"\"Importance by network centrality in correlation graph.\"\"\"\n",
    "    name = \"network\"\n",
    "    def analyze(self, panel, threshold=0.5):\n",
    "        corr = panel.corr().abs()\n",
    "        adj = (corr > threshold).astype(int).values\n",
    "        np.fill_diagonal(adj, 0)\n",
    "        \n",
    "        # Degree centrality\n",
    "        degree = adj.sum(axis=1)\n",
    "        \n",
    "        # Eigenvector centrality (dominant eigenvector)\n",
    "        try:\n",
    "            eigs, vecs = np.linalg.eig(adj.astype(float))\n",
    "            idx = np.argmax(eigs.real)\n",
    "            eigen_cent = np.abs(vecs[:, idx].real)\n",
    "        except:\n",
    "            eigen_cent = degree\n",
    "        \n",
    "        # Combine\n",
    "        combined = 0.5 * degree / (degree.max() + 1e-10) + 0.5 * eigen_cent / (eigen_cent.max() + 1e-10)\n",
    "        importance = pd.Series(combined, index=panel.columns)\n",
    "        return {'importance': importance, 'degree': pd.Series(degree, index=panel.columns)}\n",
    "\n",
    "\n",
    "# --- LENS 11: REGIME SWITCHING ---\n",
    "class RegimeSwitchingLens(BaseLens):\n",
    "    \"\"\"Importance by behavior difference across regimes.\"\"\"\n",
    "    name = \"regime\"\n",
    "    def analyze(self, panel, n_regimes=2):\n",
    "        # Simple regime detection via rolling volatility of first PC\n",
    "        X = ((panel - panel.mean()) / panel.std()).fillna(0).values\n",
    "        U, S, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "        pc1 = U[:, 0] * S[0]\n",
    "        \n",
    "        # Regime = high vol vs low vol\n",
    "        vol = pd.Series(pc1).rolling(20).std().fillna(0).values\n",
    "        threshold = np.median(vol)\n",
    "        regime = (vol > threshold).astype(int)\n",
    "        \n",
    "        # Importance: how different is behavior across regimes\n",
    "        importance_vals = []\n",
    "        for col in panel.columns:\n",
    "            s = panel[col].values\n",
    "            mean_r0 = s[regime == 0].mean() if (regime == 0).sum() > 0 else 0\n",
    "            mean_r1 = s[regime == 1].mean() if (regime == 1).sum() > 0 else 0\n",
    "            std_pooled = s.std() + 1e-10\n",
    "            importance_vals.append(abs(mean_r1 - mean_r0) / std_pooled)\n",
    "        \n",
    "        importance = pd.Series(importance_vals, index=panel.columns)\n",
    "        return {'importance': importance, 'regime_labels': regime}\n",
    "\n",
    "\n",
    "# --- LENS 12: ANOMALY ---\n",
    "class AnomalyLens(BaseLens):\n",
    "    \"\"\"Importance by frequency of anomalous behavior.\"\"\"\n",
    "    name = \"anomaly\"\n",
    "    def analyze(self, panel, z_threshold=2.5):\n",
    "        # Z-score based anomaly detection\n",
    "        z_scores = (panel - panel.mean()) / panel.std()\n",
    "        anomaly_rate = (z_scores.abs() > z_threshold).mean()\n",
    "        importance = (anomaly_rate - anomaly_rate.min()) / (anomaly_rate.max() - anomaly_rate.min() + 1e-10)\n",
    "        return {'importance': importance, 'anomaly_rate': anomaly_rate}\n",
    "\n",
    "\n",
    "# --- LENS 13: TRANSFER ENTROPY ---\n",
    "class TransferEntropyLens(BaseLens):\n",
    "    \"\"\"Importance by information flow to other series.\"\"\"\n",
    "    name = \"transfer_entropy\"\n",
    "    def analyze(self, panel, lag=1, n_bins=10):\n",
    "        cols = panel.columns.tolist()\n",
    "        te_out = pd.Series(0.0, index=cols)  # Outgoing TE\n",
    "        \n",
    "        for source in cols:\n",
    "            x = panel[source].values\n",
    "            for target in cols:\n",
    "                if source == target: continue\n",
    "                y = panel[target].values\n",
    "                \n",
    "                # Discretize\n",
    "                x_d = np.digitize(x, np.linspace(x.min(), x.max(), n_bins))\n",
    "                y_d = np.digitize(y, np.linspace(y.min(), y.max(), n_bins))\n",
    "                \n",
    "                # TE(X->Y) ‚âà I(Y_t; X_{t-lag} | Y_{t-lag})\n",
    "                # Simplified: correlation of lagged X with Y residuals\n",
    "                if len(y) > lag:\n",
    "                    y_curr = y[lag:]\n",
    "                    y_past = y[:-lag]\n",
    "                    x_past = x[:-lag]\n",
    "                    \n",
    "                    # Residual of Y_curr given Y_past\n",
    "                    try:\n",
    "                        slope = np.cov(y_curr, y_past)[0,1] / (np.var(y_past) + 1e-10)\n",
    "                        residual = y_curr - slope * y_past\n",
    "                        te = abs(np.corrcoef(residual, x_past)[0,1])\n",
    "                        if not np.isnan(te):\n",
    "                            te_out[source] += te\n",
    "                    except: pass\n",
    "        \n",
    "        importance = (te_out - te_out.min()) / (te_out.max() - te_out.min() + 1e-10)\n",
    "        return {'importance': importance, 'transfer_entropy_out': te_out}\n",
    "\n",
    "\n",
    "# --- LENS 14: TDA (Topological Data Analysis) ---\n",
    "class TDALens(BaseLens):\n",
    "    \"\"\"Importance by topological persistence in embedded space.\"\"\"\n",
    "    name = \"tda\"\n",
    "    def analyze(self, panel, embed_dim=3, delay=5, sample_size=200):\n",
    "        importance_vals = []\n",
    "        \n",
    "        for col in panel.columns:\n",
    "            s = panel[col].dropna().values\n",
    "            if len(s) < embed_dim * delay + sample_size:\n",
    "                importance_vals.append(0)\n",
    "                continue\n",
    "            \n",
    "            # Time-delay embedding\n",
    "            n = len(s) - (embed_dim - 1) * delay\n",
    "            embedded = np.array([s[i:i + embed_dim * delay:delay] for i in range(n)])\n",
    "            \n",
    "            # Subsample for speed\n",
    "            if len(embedded) > sample_size:\n",
    "                idx = np.random.choice(len(embedded), sample_size, replace=False)\n",
    "                embedded = embedded[idx]\n",
    "            \n",
    "            # Simple persistence: range of pairwise distances (proxy for H0 persistence)\n",
    "            from scipy.spatial.distance import pdist\n",
    "            dists = pdist(embedded)\n",
    "            persistence = dists.max() - dists.min() if len(dists) > 0 else 0\n",
    "            importance_vals.append(persistence)\n",
    "        \n",
    "        importance = pd.Series(importance_vals, index=panel.columns)\n",
    "        importance = (importance - importance.min()) / (importance.max() - importance.min() + 1e-10)\n",
    "        return {'importance': importance}\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# LENS REGISTRY & RUNNER\n",
    "# ============================================================================\n",
    "\n",
    "LENSES = {\n",
    "    'magnitude': MagnitudeLens,\n",
    "    'pca': PCALens,\n",
    "    'granger': GrangerLens,\n",
    "    'dmd': DMDLens,\n",
    "    'influence': InfluenceLens,\n",
    "    'mutual_info': MutualInfoLens,\n",
    "    'clustering': ClusteringLens,\n",
    "    'decomposition': DecompositionLens,\n",
    "    'wavelet': WaveletLens,\n",
    "    'network': NetworkLens,\n",
    "    'regime': RegimeSwitchingLens,\n",
    "    'anomaly': AnomalyLens,\n",
    "    'transfer_entropy': TransferEntropyLens,\n",
    "    'tda': TDALens,\n",
    "}\n",
    "\n",
    "def run_lens(name, panel):\n",
    "    return LENSES[name]().analyze(panel)\n",
    "\n",
    "def run_all_lenses(panel, names=None, verbose=True):\n",
    "    names = names or list(LENSES.keys())\n",
    "    results = {}\n",
    "    for name in names:\n",
    "        try:\n",
    "            if verbose: print(f\"  {name}...\", end=\" \")\n",
    "            results[name] = run_lens(name, panel)\n",
    "            if verbose: print(\"‚úì\")\n",
    "        except Exception as e:\n",
    "            if verbose: print(f\"‚úó ({e})\")\n",
    "    return results\n",
    "\n",
    "def compute_consensus(results):\n",
    "    rankings = {}\n",
    "    for name, res in results.items():\n",
    "        if 'importance' in res and isinstance(res['importance'], pd.Series):\n",
    "            rankings[name] = res['importance'].rank(ascending=False)\n",
    "    if not rankings: return pd.DataFrame()\n",
    "    df = pd.DataFrame(rankings)\n",
    "    df['avg_rank'] = df.mean(axis=1)\n",
    "    df['std_rank'] = df.std(axis=1)\n",
    "    df['agreement'] = 1 / (1 + df['std_rank'])\n",
    "    return df.sort_values('avg_rank')\n",
    "\n",
    "print(f\"‚úì Loaded {len(LENSES)} lenses\")\n",
    "print(f\"  {list(LENSES.keys())}\")\n",
    "print(\"\\nüîë Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title üìä **CELL 2: LOAD DATA** { display-mode: \"form\" }\n",
    "#@markdown Loads your data panel.\n",
    "\n",
    "data_path = os.path.join(PRISM_PATH, 'data', 'raw', 'master_panel.csv')\n",
    "\n",
    "if os.path.exists(data_path):\n",
    "    panel_raw = pd.read_csv(data_path, index_col=0, parse_dates=True)\n",
    "    print(f\"‚úì Loaded {data_path}\")\n",
    "else:\n",
    "    # Build from individual files\n",
    "    print(\"Building from individual CSVs...\")\n",
    "    raw_dir = os.path.join(PRISM_PATH, 'data', 'raw')\n",
    "    dfs = {}\n",
    "    for f in os.listdir(raw_dir):\n",
    "        if f.endswith('.csv'):\n",
    "            try:\n",
    "                df = pd.read_csv(os.path.join(raw_dir, f), index_col=0, parse_dates=True)\n",
    "                name = f.replace('.csv', '').upper()\n",
    "                for col in ['Close', 'Adj Close', 'VALUE', 'Value', df.columns[0]]:\n",
    "                    if col in df.columns:\n",
    "                        dfs[name] = df[col]\n",
    "                        break\n",
    "            except: pass\n",
    "    panel_raw = pd.DataFrame(dfs)\n",
    "\n",
    "# Clean\n",
    "panel = panel_raw.ffill().bfill().dropna()\n",
    "\n",
    "print(f\"\\nüìä Panel: {panel.shape[1]} indicators √ó {panel.shape[0]} days\")\n",
    "print(f\"   {panel.index[0].strftime('%Y-%m-%d')} to {panel.index[-1].strftime('%Y-%m-%d')}\")\n",
    "print(f\"   Columns: {list(panel.columns)[:10]}{'...' if len(panel.columns) > 10 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title üöÄ **CELL 3: RUN ALL 14 LENSES** { display-mode: \"form\" }\n",
    "#@markdown This may take 1-2 minutes for large panels.\n",
    "\n",
    "print(\"Running all 14 lenses...\\n\")\n",
    "results = run_all_lenses(panel)\n",
    "\n",
    "print(f\"\\n‚úì {len(results)}/14 lenses completed\")\n",
    "\n",
    "# Consensus\n",
    "consensus = compute_consensus(results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üèÜ TOP 10 INDICATORS BY CONSENSUS\")\n",
    "print(\"=\"*60)\n",
    "for i, (ind, row) in enumerate(consensus.head(10).iterrows(), 1):\n",
    "    bar = \"‚ñà\" * int(row['agreement'] * 20)\n",
    "    print(f\"{i:2}. {ind:<15} rank={row['avg_rank']:5.1f}  agreement={row['agreement']:.2f} {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title üìà **CELL 4: VISUALIZE RESULTS** { display-mode: \"form\" }\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Top indicators\n",
    "top15 = consensus.head(15)\n",
    "colors = plt.cm.RdYlGn(top15['agreement'].values)\n",
    "axes[0,0].barh(range(len(top15)), top15['avg_rank'].values[::-1], color=colors[::-1])\n",
    "axes[0,0].set_yticks(range(len(top15)))\n",
    "axes[0,0].set_yticklabels(top15.index[::-1])\n",
    "axes[0,0].set_xlabel('Avg Rank (lower=more important)')\n",
    "axes[0,0].set_title('Top 15 Indicators')\n",
    "axes[0,0].invert_xaxis()\n",
    "\n",
    "# 2. Lens agreement heatmap\n",
    "lens_cols = [c for c in consensus.columns if c not in ['avg_rank', 'std_rank', 'agreement']]\n",
    "top10_ranks = consensus[lens_cols].head(10)\n",
    "im = axes[0,1].imshow(top10_ranks.values, cmap='RdYlGn_r', aspect='auto')\n",
    "axes[0,1].set_xticks(range(len(lens_cols)))\n",
    "axes[0,1].set_xticklabels(lens_cols, rotation=45, ha='right', fontsize=8)\n",
    "axes[0,1].set_yticks(range(len(top10_ranks)))\n",
    "axes[0,1].set_yticklabels(top10_ranks.index)\n",
    "axes[0,1].set_title('Rank by Each Lens')\n",
    "plt.colorbar(im, ax=axes[0,1])\n",
    "\n",
    "# 3. Lens correlation\n",
    "lens_corr = consensus[lens_cols].corr(method='spearman')\n",
    "im2 = axes[1,0].imshow(lens_corr, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "axes[1,0].set_xticks(range(len(lens_cols)))\n",
    "axes[1,0].set_xticklabels(lens_cols, rotation=45, ha='right', fontsize=8)\n",
    "axes[1,0].set_yticks(range(len(lens_cols)))\n",
    "axes[1,0].set_yticklabels(lens_cols, fontsize=8)\n",
    "axes[1,0].set_title('Lens Agreement (Spearman œÅ)')\n",
    "plt.colorbar(im2, ax=axes[1,0])\n",
    "\n",
    "# 4. Agreement distribution\n",
    "axes[1,1].hist(consensus['agreement'], bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[1,1].axvline(consensus['agreement'].median(), color='red', linestyle='--', label=f\"Median={consensus['agreement'].median():.2f}\")\n",
    "axes[1,1].set_xlabel('Agreement Score')\n",
    "axes[1,1].set_ylabel('Count')\n",
    "axes[1,1].set_title('Distribution of Lens Agreement')\n",
    "axes[1,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print lens correlations\n",
    "print(\"\\nLens Agreement Matrix (Spearman):\")\n",
    "print(lens_corr.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title üíæ **SAVE RESULTS** { display-mode: \"form\" }\n",
    "\n",
    "output_dir = os.path.join(PRISM_PATH, '06_output', 'latest')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "consensus.to_csv(os.path.join(output_dir, 'consensus_14lens.csv'))\n",
    "\n",
    "import json\n",
    "meta = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'n_indicators': len(panel.columns),\n",
    "    'n_days': len(panel),\n",
    "    'lenses_run': list(results.keys()),\n",
    "    'top_10': list(consensus.head(10).index),\n",
    "}\n",
    "with open(os.path.join(output_dir, 'run_14lens.json'), 'w') as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(f\"‚úì Saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîß Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore a specific lens\n",
    "lens_name = 'granger'  # Change this!\n",
    "\n",
    "result = results[lens_name]\n",
    "print(f\"\\n{lens_name.upper()} LENS\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Keys: {list(result.keys())}\")\n",
    "print(f\"\\nTop 10:\")\n",
    "print(result['importance'].sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find disagreements - where lenses differ most\n",
    "high_variance = consensus.nlargest(10, 'std_rank')[['avg_rank', 'std_rank', 'agreement']]\n",
    "print(\"\\nü§î MOST CONTESTED INDICATORS (lenses disagree)\")\n",
    "print(\"=\"*50)\n",
    "print(high_variance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
